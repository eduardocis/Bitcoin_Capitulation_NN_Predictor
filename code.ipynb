{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df54fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ba7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series(url):\n",
    "    r = requests.get(url)\n",
    "    data = r.json().get(\"values\", [])\n",
    "    df = pd.DataFrame(data)\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()  \n",
    "    \n",
    "    df[\"x\"] = pd.to_datetime(df[\"x\"], unit=\"s\")\n",
    "    df = df.rename(columns={\"x\": \"date\", \"y\": url.split(\"/\")[-1]})\n",
    "    df = df.set_index(\"date\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bad8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api calls to get various bitcoin time series data\n",
    "difficulty = get_series(\"https://api.blockchain.info/charts/difficulty?timespan=5year&sampled=true&format=json\")\n",
    "hashrate = get_series(\"https://api.blockchain.info/charts/hash-rate?timespan=5year&sampled=true&format=json\")\n",
    "price = get_series(\"https://api.blockchain.info/charts/market-price?timespan=5year&sampled=true&format=json\")\n",
    "miner_rev = get_series(\"https://api.blockchain.info/charts/miners-revenue?timespan=5year&sampled=true&format=json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a11d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all series on the date index using outer join\n",
    "df = difficulty.join([hashrate, price, miner_rev], how=\"outer\")\n",
    "\n",
    "# reindex to a **continuous daily index**\n",
    "daily_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq=\"D\")\n",
    "df = df.reindex(daily_index)\n",
    "\n",
    "# fill missing values with forward-fill\n",
    "df = df.ffill()\n",
    "\n",
    "df.columns = [\"difficulty\", \"hashrate\", \"price\", \"miner_rev_usd\"]\n",
    "\n",
    "# compute hashrate pct change\n",
    "df[\"hashrate_pct_change\"] = df[\"hashrate\"].pct_change() * 100\n",
    "\n",
    "# look at difficutly changes around hashrate crashes\n",
    "df[\"difficulty_pct_change_next\"] = df[\"difficulty\"].pct_change().shift(-1) * 100\n",
    "\n",
    "# hashprice\n",
    "df[\"hashprice\"] = df[\"miner_rev_usd\"] / df[\"hashrate\"]\n",
    "df[\"hashprice_change_next\"] = df[\"hashprice\"].pct_change().shift(-1) * 100\n",
    "\n",
    "df['hashrate_ma30'] = df['hashrate'].rolling(30).mean()\n",
    "df['hashrate_ma60'] = df['hashrate'].rolling(60).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82335869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lagged features (what happened recently matters)\n",
    "df['hashrate_lag1'] = df['hashrate'].shift(1)\n",
    "df['hashrate_lag7'] = df['hashrate'].shift(7)\n",
    "df['hashrate_lag14'] = df['hashrate'].shift(14)\n",
    "df['hashprice_lag1'] = df['hashprice'].shift(1)\n",
    "df['price_lag1'] = df['price'].shift(1)\n",
    "\n",
    "# 2. Percentage changes (momentum indicators)\n",
    "df['difficulty_pct_change'] = df['difficulty'].pct_change() * 100\n",
    "df['price_pct_change'] = df['price'].pct_change() * 100\n",
    "df['hashprice_pct_change'] = df['hashprice'].pct_change() * 100\n",
    "\n",
    "# 3. Rolling statistics (trend detection)\n",
    "df['hashrate_ma7'] = df['hashrate'].rolling(7).mean()\n",
    "df['hashrate_ma14'] = df['hashrate'].rolling(14).mean()\n",
    "df['price_ma30'] = df['price'].rolling(30).mean()\n",
    "df['hashprice_ma7'] = df['hashprice'].rolling(7).mean()\n",
    "\n",
    "# 4. Volatility (market stress indicator)\n",
    "df['hashrate_volatility_7d'] = df['hashrate_pct_change'].rolling(7).std()\n",
    "df['price_volatility_7d'] = df['price_pct_change'].rolling(7).std()\n",
    "df['hashprice_volatility_7d'] = df['hashprice_pct_change'].rolling(7).std()\n",
    "\n",
    "# 5. Distance from moving averages (divergence indicator)\n",
    "df['hashrate_distance_from_ma30'] = ((df['hashrate'] - df['hashrate_ma30']) / df['hashrate_ma30']) * 100\n",
    "df['hashrate_distance_from_ma60'] = ((df['hashrate'] - df['hashrate_ma60']) / df['hashrate_ma60']) * 100\n",
    "\n",
    "# 6. Cumulative metrics (how long has stress lasted?)\n",
    "df['days_below_ma60'] = (df['hashrate'] < df['hashrate_ma60']).astype(int).rolling(30).sum()\n",
    "df['consecutive_decline_days'] = (df['hashrate_pct_change'] < 0).astype(int)\n",
    "df['consecutive_decline_days'] = df['consecutive_decline_days'].groupby((df['consecutive_decline_days'] != df['consecutive_decline_days'].shift()).cumsum()).cumsum()\n",
    "\n",
    "# 7. Difficulty adjustment anticipation\n",
    "df['blocks_since_adjustment'] = (df.index - df.index[0]).days % 14  # ~14 days per adjustment\n",
    "\n",
    "# 8. Flag for hash ribbons\n",
    "df['hash_ribbon'] = (df['hashrate_ma30'] < df['hashrate_ma60']).astype(int) # hash ribbons, if 30ma < 60ma, then 1 else 0\n",
    "\n",
    "# 9. Price-to-hashprice ratio (economic pressure indicator)\n",
    "df['price_hashprice_ratio'] = df['price'] / df['hashprice']\n",
    "\n",
    "# Target: predict capitulation 3 days ahead using hash_ribbon\n",
    "df['target'] = df['hash_ribbon'].shift(-3)\n",
    "\n",
    "df = df.dropna()  # drop rows with NaN values created by feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e232aec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 11\n",
      "Target distribution: Capitulation=347.0, Recovery=1416.0\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    # Core Hash Ribbons components\n",
    "    'hashrate_ma30', 'hashrate_ma60',\n",
    "    'hashrate_distance_from_ma30', 'hashrate_distance_from_ma60',\n",
    "    \n",
    "    # Recent changes (momentum)\n",
    "    'hashrate_pct_change', 'hashprice_pct_change',\n",
    "    \n",
    "    # Stress indicators\n",
    "    'days_below_ma60', 'consecutive_decline_days',\n",
    "    \n",
    "    # Volatility\n",
    "    'hashrate_volatility_7d',\n",
    "    \n",
    "    # Lags\n",
    "    'hashrate_lag1', 'hashrate_lag7'\n",
    "]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df['target'].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target distribution: Capitulation={y.sum()}, Recovery={len(y)-y.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f249bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapitulationDataset(Dataset):\n",
    "    def __init__(self, X, y, sequence_length=45):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.sequence_length]\n",
    "        y_label = self.y[idx+self.sequence_length]\n",
    "        return torch.FloatTensor(X_seq), torch.FloatTensor([y_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56bdf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 1374, Test: 344\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 45\n",
    "dataset = CapitulationDataset(X_scaled, y, sequence_length)\n",
    "\n",
    "# Split\n",
    "split = int(len(dataset) * 0.8)\n",
    "train_indices = list(range(split))\n",
    "test_indices = list(range(split, len(dataset)))\n",
    "\n",
    "# ============================================ \n",
    "# WEIGHTED SAMPLER FOR TRAIN SET (OVERSAMPLE MINORITY CLASS)\n",
    "# ============================================ \n",
    "\n",
    "# Get labels for training set\n",
    "train_labels = []\n",
    "for idx in train_indices:\n",
    "    _, label = dataset[idx]\n",
    "    train_labels.append(label.item())\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Calculate sample weights (inverse frequency)\n",
    "class_counts = np.bincount(train_labels.astype(int))\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = class_weights[train_labels.astype(int)]\n",
    "\n",
    "# Create weighted sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "# Use sampler for training (this will oversample minority class)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2e82d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapitulationGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=32, dropout=0.3):\n",
    "        super(CapitulationGRU, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        out = self.dropout(last_output)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CapitulationGRU(input_size=len(feature_cols), hidden_size=32, dropout=0.3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "538d7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "criterion = FocalLoss(alpha=0.8, gamma=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e122dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "Epoch 10 - Train Loss: 0.0180, Val Loss: 0.0803, Val F1: 0.5366\n",
      "Epoch 20 - Train Loss: 0.0098, Val Loss: 0.1059, Val F1: 0.5185\n",
      "Epoch 30 - Train Loss: 0.0080, Val Loss: 0.1569, Val F1: 0.4211\n",
      "Early stopping at epoch 39\n",
      "\n",
      " Training complete! Best F1: 0.6452\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_f1 = 0\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions for F1 calculation\n",
    "            preds = (outputs.cpu().numpy() > 0.5).astype(int).flatten()\n",
    "            val_preds.extend(preds)\n",
    "            val_actuals.extend(y_batch.cpu().numpy().flatten().astype(int))\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    from sklearn.metrics import f1_score\n",
    "    val_f1 = f1_score(val_actuals, val_preds, zero_division=0)\n",
    "    \n",
    "    # Early stopping based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_f1.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model_f1.pth'))\n",
    "print(f\"\\n Training complete! Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df7f86f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal threshold for F1: 0.422\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL PERFORMANCE\n",
      "============================================================\n",
      "Accuracy: 0.9157\n",
      "Precision: 0.8444\n",
      "Recall: 0.6333\n",
      "F1-Score: 0.7238\n",
      "ROC-AUC: 0.9192\n",
      "\n",
      "Confusion Matrix:\n",
      "[[277   7]\n",
      " [ 22  38]]\n",
      "TN: 277, FP: 7\n",
      "FN: 22, TP: 38\n",
      "\n",
      "Class distribution in predictions:\n",
      "Predicted Capitulation: 45 (13.1%)\n",
      "Predicted Recovery: 299 (86.9%)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "probs = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        probs.extend(outputs.cpu().numpy().flatten())\n",
    "        actuals.extend(y_batch.numpy().flatten())\n",
    "\n",
    "probs = np.array(probs)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(actuals, probs)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "\n",
    "print(f\"\\nOptimal threshold for F1: {optimal_threshold:.3f}\")\n",
    "\n",
    "# Predictions with optimal threshold\n",
    "predictions = (probs > optimal_threshold).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(actuals, predictions):.4f}\")\n",
    "print(f\"Precision: {precision_score(actuals, predictions, zero_division=0):.4f}\")\n",
    "print(f\"Recall: {recall_score(actuals, predictions, zero_division=0):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(actuals, predictions, zero_division=0):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(actuals, probs):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(actuals, predictions)\n",
    "print(cm)\n",
    "print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "print(f\"FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "\n",
    "print(\"\\nClass distribution in predictions:\")\n",
    "print(f\"Predicted Capitulation: {predictions.sum()} ({predictions.sum()/len(predictions)*100:.1f}%)\")\n",
    "print(f\"Predicted Recovery: {len(predictions)-predictions.sum()} ({(len(predictions)-predictions.sum())/len(predictions)*100:.1f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
